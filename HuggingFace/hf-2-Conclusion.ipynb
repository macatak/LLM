{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c29f7f-8f67-4b99-931b-3e4a33074422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion for using transformers\n",
    "# based on https://huggingface.co/learn/nlp-course/chapter2/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88441834-2722-4b9b-892a-e8c64baa97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"Arthur, who didn't know, found out Ford and Zaphod were cousins.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d750dd9b-f6f0-4cc9-a01b-840558a5137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad several sequences\n",
    "sequences = \"Arthur, who didn't know, found out Ford and Zaphod were cousins.\"\n",
    "\n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c4cde0-3118-45fd-8096-9aa51ebe5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6686991-cb7a-4dd8-9acd-969987313757",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Arthur, who didn't know, found out Ford and Zaphod were cousins.\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1050ca77-1e4b-4142-995d-86db0ced5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = [\"Arthur, who didn't know, found out Ford and Zaphod were cousins.\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9735e52-8d7a-4741-b7ad-24ee49a1577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47874382-36bf-48cf-a9df-86b2b8d686ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] arthur, who didn't know, found out ford and zaphod were cousins. [SEP]\n",
      "arthur, who didn't know, found out ford and zaphod were cousins.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c7ce623-25b6-4623-80e5-a9cbc15ec319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : {'input_ids': tensor([[  101,  4300,  1010,  2040,  2134,  1005,  1056,  2113,  1010,  2179,\n",
      "          2041,  4811,  1998, 23564,  8458,  7716,  2020, 12334,  1012,  1012,\n",
      "           102],\n",
      "        [  101, 13012,  6894,  2319,  2354,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "output : SequenceClassifierOutput(loss=None, logits=tensor([[-1.8782,  1.8612],\n",
      "        [-2.1634,  2.1248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"Arthur, who didn't know, found out Ford and Zaphod were cousins..\", \"Trillian knew\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Tokens : {}\".format(tokens))\n",
    "\n",
    "output = model(**tokens)\n",
    "print(\"output : {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221d942-8f4f-42b1-94da-26b6d1bc2394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
