{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ea89d6-d598-41ba-b44f-8ac83888fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1385136-c2a9-4150-8fc3-88e41c4d930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9995536208152771},\n",
       " {'label': 'POSITIVE', 'score': 0.9973645806312561},\n",
       " {'label': 'NEGATIVE', 'score': 0.9924862384796143}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", device=0)\n",
    "classifier(\n",
    "    [\n",
    "    \"I love cats\",\n",
    "    \"I like dogs better than cats\",\n",
    "    \"I do not like skunks\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "570ebf06-4b25-460f-8975-bf28bf50408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1474a179-1ff3-41e4-9e60-3b432417261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ce2162c6-5505-46ce-9cb1-52ee2bbfe964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is a dictionary with a key for each input: input_ids and the attention mask\n",
      "input_ids contains a row of integers, one for each input, which are the uniqie IDs for the tokens in each input\n",
      "attention mask is TBD on info\n",
      "{'input_ids': tensor([[  101,  1045,  2293,  8870,   102,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2066,  6077,  2488,  2084,  8870,   102,     0],\n",
      "        [  101,  1045,  2079,  2025,  2066, 15315, 16814,  2015,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I love cats\",\n",
    "    \"I like dogs better than cats\",\n",
    "    \"I do not like skunks\"\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Output is a dictionary with a key for each input: input_ids and the attention mask\")\n",
    "print(\"input_ids contains a row of integers, one for each input, which are the uniqie IDs for the tokens in each input\")\n",
    "print(\"attention mask is TBD on info\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d84db6af-fe2a-422d-a35a-02dc7c7489f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go through the model\n"
     ]
    }
   ],
   "source": [
    "print(\"Go through the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad26389a-e151-4f69-88a2-14a5428dd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "# download the checkpoint\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# instantiate a model\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "595204d8-28fb-4651-81bf-b9dddde236f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "# use the same input sentences from above\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00659aac-1d0a-400a-bda0-0943969f252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "the 2 values are because we have 2 sentences and 2 labels\n"
     ]
    }
   ],
   "source": [
    "# same but using the model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)\n",
    "print(\"the 2 values are because we have 2 sentences and 2 labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59b86f1d-d1dd-40c2-96d7-768d6e55a5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 2 values are the for the 1st sentence, and the 2nd values are the 2nd sentence\n",
      "not probabilites but logits which are raw unnormalized scores from the models last layer\n",
      "Softmax layer normalizes them\n",
      "tensor([[-3.7413,  3.9726],\n",
      "        [-2.9056,  3.0305],\n",
      "        [ 2.6433, -2.2401]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# logits\n",
    "print(\"get a value for each input sentence\")\n",
    "print(\"not probabilites but logits which are raw unnormalized scores from the models last layer\")\n",
    "print(\"Softmax layer normalizes them\")\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05225b2f-2dcb-45c6-8b24-5cdea253db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert logits to probabilities\n",
      "tensor([[4.4638e-04, 9.9955e-01],\n",
      "        [2.6355e-03, 9.9736e-01],\n",
      "        [9.9249e-01, 7.5138e-03]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"convert logits to probabilities\")\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b876a051-1c45-4a5c-b882-b1e0da323652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print the labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"print the labels\")\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97233965-b9e4-4146-8a4c-f666db14a058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
