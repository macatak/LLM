{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8424c248-f7a1-4585-8430-3312e52c7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing on the GLUE SST-2 dataset\n",
    "# composed of single sentences, not pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee925f8b-b011-48d4-b34e-28d1dbf859bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a DatasetDict object which contains the training set, the validation set, and the test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the MRPC\n",
    "print(\"get a DatasetDict object which contains the training set, the validation set, and the test set\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64c84a9-bd14-4902-9e8f-acc9e5e5ebe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access each sentence in the raw_datasets object by indexing, like with a dictionary\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04c3144-4969-4f92-bf9c-f59387cda32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the names=['negative', 'positive'] indicates the values: 0 is negative, 1 is positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"the names=['negative', 'positive'] indicates the values: 0 is negative, 1 is positive\")\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beafaedc-6263-4596-a959-5c695a230aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print element 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'the greatest musicians ', 'label': 1, 'idx': 15}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "print(\"print element 15\")\n",
    "raw_train_dataset[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb6aac9-1a15-4b63-846d-1ae9dd7285dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print element 87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'khouri manages , with terrific flair , to keep the extremes of screwball farce and blood-curdling family intensity on one continuum . ',\n",
       " 'label': 1,\n",
       " 'idx': 87}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"print element 87\")\n",
    "raw_train_dataset[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f9034b8-8b3d-4f35-b096-e9e5d461df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_type_ids indicate which sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the sentences, not a good approach\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence\"])\n",
    "print(\"token_type_ids indicate which sentence\")\n",
    "inputs = tokenizer(\"This is the first sentence.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39449955-89b2-4392-83d8-443361cf42b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode the IDs\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4794b018-5267-4dd2-b1a7-f06f7e3e723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize entire dataset\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d925e3-0b00-463a-95e0-ee2763ec9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd408e2f-6f9d-4c05-86f3-e2eb1801d0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "047e37b2-5f1b-434a-ade0-d5020942afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic padding -  pad all the examples to the length of the longest element \n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a6560f-591c-4d9f-be49-3e4d1165548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 15, 10, 22, 13, 29, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look and see what the longest length is by sampling so guessing\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a39c672-1f5f-4703-8cea-cc01ddf60b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 29]),\n",
       " 'token_type_ids': torch.Size([8, 29]),\n",
       " 'attention_mask': torch.Size([8, 29]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b95ce-7bc6-4c55-8169-5e3253f5b7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
