{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd09e9e0-04c2-475d-ad8c-bee0c48e2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081c216-e535-4269-8488-c4d4bbfb7c52",
   "metadata": {},
   "source": [
    "## LLM service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8428f994-c71b-4d7f-9207-33409b6e8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama default: local service\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# OpenAI-compatible defaults:\n",
    "OPENAI_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\", \"https://api.openai.com\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8de78d-c52f-409d-b33f-614035fef9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_openai_credentials(base_url=None, api_key=None):\n",
    "    \"\"\"\n",
    "    Optionally update the OpenAI connection info at runtime.\n",
    "    \"\"\"\n",
    "    global OPENAI_BASE_URL, OPENAI_API_KEY\n",
    "    if base_url:\n",
    "        OPENAI_BASE_URL = base_url.rstrip(\"/\")\n",
    "    if api_key is not None:\n",
    "        OPENAI_API_KEY = api_key\n",
    "\n",
    "def verify_ollama_connection(base_url=None, timeout=5):\n",
    "    \"\"\"\n",
    "    Verify Ollama by calling /api/tags.\n",
    "    \"\"\"\n",
    "    base = (base_url or OLLAMA_BASE_URL).rstrip(\"/\")\n",
    "    url = f\"{base}/api/tags\"\n",
    "    try:\n",
    "        req = urllib.request.Request(url, method=\"GET\")\n",
    "        with urllib.request.urlopen(req, timeout=timeout) as resp:\n",
    "            return {\"ok\": True, \"status\": resp.status, \"provider\": \"ollama\"}\n",
    "    except urllib.error.URLError as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"provider\": \"ollama\"}\n",
    "\n",
    "def verify_openai_connection(base_url=None, api_key=None, timeout=5):\n",
    "    \"\"\"\n",
    "    Verify OpenAI-compatible endpoint by calling /v1/models.\n",
    "    Requires a valid API key on endpoints that enforce auth.\n",
    "    \"\"\"\n",
    "    base = (base_url or OPENAI_BASE_URL).rstrip(\"/\")\n",
    "    key = OPENAI_API_KEY if api_key is None else api_key\n",
    "    url = f\"{base}/v1/models\"\n",
    "    try:\n",
    "        req = urllib.request.Request(url, method=\"GET\", headers={\n",
    "            \"Authorization\": f\"Bearer {key}\" if key else \"\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        })\n",
    "        with urllib.request.urlopen(req, timeout=timeout) as resp:\n",
    "            return {\"ok\": True, \"status\": resp.status, \"provider\": \"openai\"}\n",
    "    except urllib.error.URLError as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"provider\": \"openai\"}\n",
    "\n",
    "# simple connect check\n",
    "def verify_ollama_connection(base_url=\"http://localhost:11434\", timeout=5):\n",
    "    url = base_url.rstrip(\"/\") + \"/api/tags\"\n",
    "    try:\n",
    "        req = urllib.request.Request(url, method=\"GET\")\n",
    "        with urllib.request.urlopen(req, timeout=timeout) as resp:\n",
    "            return {\"ok\": True, \"status\": resp.status}\n",
    "    except urllib.error.URLError as e:\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "\n",
    "def set_openai_credentials(base_url=None, api_key=None):\n",
    "    \"\"\"\n",
    "    Optionally update the OpenAI connection info at runtime.\n",
    "    \"\"\"\n",
    "    global OPENAI_BASE_URL, OPENAI_API_KEY\n",
    "    if base_url:\n",
    "        OPENAI_BASE_URL = base_url.rstrip(\"/\")\n",
    "    if api_key is not None:\n",
    "        OPENAI_API_KEY = api_key\n",
    "        \n",
    "\n",
    "# Start a structure to hold model metadata\n",
    "def initialize_model_registry(model_names):\n",
    "    \"\"\"\n",
    "    Given a list of model names, return a dict where each\n",
    "    model is a key and its value is an empty dict (to be filled later).\n",
    "    \"\"\"\n",
    "    return {name: {} for name in model_names}\n",
    "\n",
    "\n",
    "def build_model_registry(base_url=\"http://localhost:11434\", timeout=5):\n",
    "    \"\"\"\n",
    "    Query Ollama for local models and return a dict with useful metadata.\n",
    "    Keys are model names; values are plain JSON-serializable dicts.\n",
    "    \"\"\"\n",
    "    base = base_url.rstrip(\"/\")\n",
    "    # 1) Get local models\n",
    "    with urllib.request.urlopen(f\"{base}/api/tags\", timeout=timeout) as resp:\n",
    "        tags = json.loads(resp.read().decode(\"utf-8\"))\n",
    "    models = tags.get(\"models\", [])\n",
    "\n",
    "    registry = {}\n",
    "\n",
    "    def _post_json(url, payload):\n",
    "        data = json.dumps(payload).encode(\"utf-8\")\n",
    "        req = urllib.request.Request(url, data=data, headers={\"Content-Type\": \"application/json\"})\n",
    "        with urllib.request.urlopen(req, timeout=timeout) as resp:\n",
    "            return json.loads(resp.read().decode(\"utf-8\"))\n",
    "\n",
    "    for m in models:\n",
    "        name = (m or {}).get(\"name\")\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        # Base fields from /api/tags\n",
    "        details = (m or {}).get(\"details\") or {}\n",
    "        entry = {\n",
    "            \"name\": name,\n",
    "            \"modified_at\": m.get(\"modified_at\"),\n",
    "            \"size_bytes\": m.get(\"size\"),\n",
    "            \"digest\": m.get(\"digest\"),\n",
    "            \"format\": details.get(\"format\"),\n",
    "            \"family\": details.get(\"family\"),\n",
    "            \"families\": details.get(\"families\"),\n",
    "            \"parameter_size\": details.get(\"parameter_size\"),       # e.g., \"7B\", \"13B\"\n",
    "            \"quantization\": details.get(\"quantization_level\"),     # e.g., \"Q4_0\"\n",
    "        }\n",
    "\n",
    "        # Enrich via /api/show\n",
    "        try:\n",
    "            shown = _post_json(f\"{base}/api/show\", {\"model\": name})\n",
    "        except urllib.error.URLError:\n",
    "            registry[name] = entry\n",
    "            continue\n",
    "\n",
    "        model_info = shown.get(\"model_info\") or {}\n",
    "        parameters_blob = shown.get(\"parameters\") or \"\"\n",
    "\n",
    "        # Parameter count\n",
    "        param_count = model_info.get(\"general.parameter_count\")\n",
    "        if isinstance(param_count, int):\n",
    "            entry[\"parameter_count\"] = param_count\n",
    "\n",
    "        # Context length\n",
    "        ctx = None\n",
    "        for k, v in model_info.items():\n",
    "            if isinstance(k, str) and k.endswith(\".context_length\") and isinstance(v, int):\n",
    "                ctx = v\n",
    "                break\n",
    "        if ctx is None and isinstance(parameters_blob, str):\n",
    "            m_ctx = re.search(r\"\\bnum_ctx\\s+(\\d+)\", parameters_blob)\n",
    "            if m_ctx:\n",
    "                ctx = int(m_ctx.group(1))\n",
    "        entry[\"context_length\"] = ctx\n",
    "\n",
    "        registry[name] = entry\n",
    "\n",
    "    return registry\n",
    "\n",
    "def enrich_model_registry(registry):\n",
    "    \"\"\"\n",
    "    Add extra fields useful for recommendations.\n",
    "    Defaults are provided where info is missing so you can easily update later.\n",
    "    \"\"\"\n",
    "    for name, entry in registry.items():\n",
    "        # --- Defaults section ---\n",
    "        # If the API does not return these, we set safe defaults.\n",
    "        entry.setdefault(\"instruction_tuned\", True)   # assume instruction tuned unless you know it's a base model\n",
    "        entry.setdefault(\"task_specialty\", [\"general\"])  # can update per model (e.g., [\"code\"], [\"math\"])\n",
    "        entry.setdefault(\"release_date\", \"unknown\")   # placeholder, fill with year/month if known\n",
    "        entry.setdefault(\"publisher\", \"unknown\")      # e.g., Meta, Mistral AI, etc.\n",
    "        entry.setdefault(\"license\", \"unknown\")        # e.g., Apache 2.0, commercial-restricted\n",
    "        # --- End defaults section ---\n",
    "\n",
    "        # Normalize parameter_count if it's missing but parameter_size is present\n",
    "        if \"parameter_count\" not in entry and \"parameter_size\" in entry:\n",
    "            size_str = entry[\"parameter_size\"].lower()\n",
    "            if size_str.endswith(\"b\"):  # handle like \"7b\", \"13b\"\n",
    "                try:\n",
    "                    entry[\"parameter_count\"] = int(float(size_str[:-1]) * 1e9)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    return registry\n",
    "\n",
    "def registry_to_dataframe(registry):\n",
    "    \"\"\"\n",
    "    Convert the model registry dict into a Pandas DataFrame.\n",
    "    Each key in the registry becomes a row.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(registry, orient=\"index\")\n",
    "    df.index.name = \"model\"\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b371f2c0-454e-4f3d-b9c9-5173862e514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': True, 'status': 200}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the check\n",
    "verify_ollama_connection()\n",
    "# verify_openai_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fcbd7f-3101-46ab-afcf-a383fc44d070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama3.1:8b': {'name': 'llama3.1:8b',\n",
       "  'modified_at': '2025-09-03T15:23:37.281393299-04:00',\n",
       "  'size_bytes': 4920753328,\n",
       "  'digest': '46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '8.0B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 8030261312,\n",
       "  'context_length': 131072},\n",
       " 'llama2-uncensored:latest': {'name': 'llama2-uncensored:latest',\n",
       "  'modified_at': '2025-08-12T09:12:54.355064054-04:00',\n",
       "  'size_bytes': 3825819449,\n",
       "  'digest': '44040b9222331f7eacd27ec9254e42de585af28d2c5d1211cdaeb3ffa361fe3f',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': None,\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 6738415616,\n",
       "  'context_length': 2048},\n",
       " 'whisper:latest': {'name': 'whisper:latest',\n",
       "  'modified_at': '2025-07-22T19:00:46.141884565-04:00',\n",
       "  'size_bytes': 44039248,\n",
       "  'digest': '9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81b6dcc455efeea6b8b0',\n",
       "  'format': 'gguf',\n",
       "  'family': 'unknown',\n",
       "  'families': ['unknown'],\n",
       "  'parameter_size': '37.76M',\n",
       "  'quantization': 'unknown',\n",
       "  'parameter_count': 37760640,\n",
       "  'context_length': None},\n",
       " 'dimavz/whisper-tiny:latest': {'name': 'dimavz/whisper-tiny:latest',\n",
       "  'modified_at': '2025-07-22T18:50:09.444127962-04:00',\n",
       "  'size_bytes': 44039248,\n",
       "  'digest': '9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81b6dcc455efeea6b8b0',\n",
       "  'format': 'gguf',\n",
       "  'family': 'unknown',\n",
       "  'families': ['unknown'],\n",
       "  'parameter_size': '37.76M',\n",
       "  'quantization': 'unknown',\n",
       "  'parameter_count': 37760640,\n",
       "  'context_length': None},\n",
       " 'openchat:latest': {'name': 'openchat:latest',\n",
       "  'modified_at': '2025-07-14T18:40:40.971119981-04:00',\n",
       "  'size_bytes': 4109876386,\n",
       "  'digest': '537a4e03b649d93bf57381199a85f412bfc35912e46db197407740230968e71f',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 7241748480,\n",
       "  'context_length': 8192},\n",
       " 'phi3:mini': {'name': 'phi3:mini',\n",
       "  'modified_at': '2025-07-13T17:18:01.565393789-04:00',\n",
       "  'size_bytes': 2176178913,\n",
       "  'digest': '4f222292793889a9a40a020799cfd28d53f3e01af25d48e06c5e708610fc47e9',\n",
       "  'format': 'gguf',\n",
       "  'family': 'phi3',\n",
       "  'families': ['phi3'],\n",
       "  'parameter_size': '3.8B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 3821079648,\n",
       "  'context_length': 131072},\n",
       " 'deepseek-coder-v2:latest': {'name': 'deepseek-coder-v2:latest',\n",
       "  'modified_at': '2025-07-09T18:49:47.931980663-04:00',\n",
       "  'size_bytes': 8905126121,\n",
       "  'digest': '63fb193b3a9b4322a18e8c6b250ca2e70a5ff531e962dbf95ba089b2566f2fa5',\n",
       "  'format': 'gguf',\n",
       "  'family': 'deepseek2',\n",
       "  'families': ['deepseek2'],\n",
       "  'parameter_size': '15.7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 15706484224,\n",
       "  'context_length': 163840},\n",
       " 'llama2:13b': {'name': 'llama2:13b',\n",
       "  'modified_at': '2025-05-05T18:57:06.208582485-04:00',\n",
       "  'size_bytes': 7366821294,\n",
       "  'digest': 'd475bf4c50bc4d29f333023e38cd56535039eec11052204e5304c8773cc8416c',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '13B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 13015864320,\n",
       "  'context_length': 4096},\n",
       " 'llama2:latest': {'name': 'llama2:latest',\n",
       "  'modified_at': '2025-05-05T18:51:57.858011224-04:00',\n",
       "  'size_bytes': 3826793677,\n",
       "  'digest': '78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 6738415616,\n",
       "  'context_length': 4096},\n",
       " 'mistral:latest': {'name': 'mistral:latest',\n",
       "  'modified_at': '2025-04-27T14:40:11.242436536-04:00',\n",
       "  'size_bytes': 4113301824,\n",
       "  'digest': 'f974a74358d62a017b37c6f424fcdf2744ca02926c4f952513ddf474b2fa5091',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7.2B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 7248023552,\n",
       "  'context_length': 32768},\n",
       " 'llama3.2:latest': {'name': 'llama3.2:latest',\n",
       "  'modified_at': '2025-04-21T15:58:20.764949178-04:00',\n",
       "  'size_bytes': 2019393189,\n",
       "  'digest': 'a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '3.2B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 3212749888,\n",
       "  'context_length': 131072},\n",
       " 'llama3.1:latest': {'name': 'llama3.1:latest',\n",
       "  'modified_at': '2025-04-21T15:53:22.687678259-04:00',\n",
       "  'size_bytes': 4920753328,\n",
       "  'digest': '46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '8.0B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 8030261312,\n",
       "  'context_length': 131072},\n",
       " 'llava:latest': {'name': 'llava:latest',\n",
       "  'modified_at': '2025-04-20T15:41:19.42622029-04:00',\n",
       "  'size_bytes': 4733363377,\n",
       "  'digest': '8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama', 'clip'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 7241732096,\n",
       "  'context_length': 32768},\n",
       " 'mannix/llama3-uncensored:latest': {'name': 'mannix/llama3-uncensored:latest',\n",
       "  'modified_at': '2025-04-20T15:27:40.375969333-04:00',\n",
       "  'size_bytes': 4661225247,\n",
       "  'digest': '2a46886ded540156e538542414cb1abecd15d2752b30c7125993883582760600',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '8B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 8030261248,\n",
       "  'context_length': 8192},\n",
       " 'gemma3:12b': {'name': 'gemma3:12b',\n",
       "  'modified_at': '2025-04-20T15:24:14.131208347-04:00',\n",
       "  'size_bytes': 8149190253,\n",
       "  'digest': 'f4031aab637d1ffa37b42570452ae0e4fad0314754d17ded67322e4b95836f8a',\n",
       "  'format': 'gguf',\n",
       "  'family': 'gemma3',\n",
       "  'families': ['gemma3'],\n",
       "  'parameter_size': '12.2B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 12187079280,\n",
       "  'context_length': 131072},\n",
       " 'gemma3:1b': {'name': 'gemma3:1b',\n",
       "  'modified_at': '2025-04-20T15:19:36.679311687-04:00',\n",
       "  'size_bytes': 815319791,\n",
       "  'digest': '8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc',\n",
       "  'format': 'gguf',\n",
       "  'family': 'gemma3',\n",
       "  'families': ['gemma3'],\n",
       "  'parameter_size': '999.89M',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 999885952,\n",
       "  'context_length': 32768},\n",
       " 'gemma3:latest': {'name': 'gemma3:latest',\n",
       "  'modified_at': '2025-04-20T15:19:01.77545181-04:00',\n",
       "  'size_bytes': 3338801804,\n",
       "  'digest': 'a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a',\n",
       "  'format': 'gguf',\n",
       "  'family': 'gemma3',\n",
       "  'families': ['gemma3'],\n",
       "  'parameter_size': '4.3B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 4299915632,\n",
       "  'context_length': 131072}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry = build_model_registry()\n",
    "model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7de029-fe5f-4a99-b1bc-b8875b7f7aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>name</th>\n",
       "      <th>modified_at</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>digest</th>\n",
       "      <th>format</th>\n",
       "      <th>family</th>\n",
       "      <th>families</th>\n",
       "      <th>parameter_size</th>\n",
       "      <th>quantization</th>\n",
       "      <th>parameter_count</th>\n",
       "      <th>context_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>2025-09-03T15:23:37.281393299-04:00</td>\n",
       "      <td>4920753328</td>\n",
       "      <td>46e0c10c039e019119339687c3c1757cc81b9da49709a3...</td>\n",
       "      <td>gguf</td>\n",
       "      <td>llama</td>\n",
       "      <td>[llama]</td>\n",
       "      <td>8.0B</td>\n",
       "      <td>Q4_K_M</td>\n",
       "      <td>8030261312</td>\n",
       "      <td>131072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-uncensored:latest</td>\n",
       "      <td>llama2-uncensored:latest</td>\n",
       "      <td>2025-08-12T09:12:54.355064054-04:00</td>\n",
       "      <td>3825819449</td>\n",
       "      <td>44040b9222331f7eacd27ec9254e42de585af28d2c5d12...</td>\n",
       "      <td>gguf</td>\n",
       "      <td>llama</td>\n",
       "      <td>None</td>\n",
       "      <td>7B</td>\n",
       "      <td>Q4_0</td>\n",
       "      <td>6738415616</td>\n",
       "      <td>2048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whisper:latest</td>\n",
       "      <td>whisper:latest</td>\n",
       "      <td>2025-07-22T19:00:46.141884565-04:00</td>\n",
       "      <td>44039248</td>\n",
       "      <td>9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81...</td>\n",
       "      <td>gguf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[unknown]</td>\n",
       "      <td>37.76M</td>\n",
       "      <td>unknown</td>\n",
       "      <td>37760640</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dimavz/whisper-tiny:latest</td>\n",
       "      <td>dimavz/whisper-tiny:latest</td>\n",
       "      <td>2025-07-22T18:50:09.444127962-04:00</td>\n",
       "      <td>44039248</td>\n",
       "      <td>9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81...</td>\n",
       "      <td>gguf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[unknown]</td>\n",
       "      <td>37.76M</td>\n",
       "      <td>unknown</td>\n",
       "      <td>37760640</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openchat:latest</td>\n",
       "      <td>openchat:latest</td>\n",
       "      <td>2025-07-14T18:40:40.971119981-04:00</td>\n",
       "      <td>4109876386</td>\n",
       "      <td>537a4e03b649d93bf57381199a85f412bfc35912e46db1...</td>\n",
       "      <td>gguf</td>\n",
       "      <td>llama</td>\n",
       "      <td>[llama]</td>\n",
       "      <td>7B</td>\n",
       "      <td>Q4_0</td>\n",
       "      <td>7241748480</td>\n",
       "      <td>8192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model                        name  \\\n",
       "0                 llama3.1:8b                 llama3.1:8b   \n",
       "1    llama2-uncensored:latest    llama2-uncensored:latest   \n",
       "2              whisper:latest              whisper:latest   \n",
       "3  dimavz/whisper-tiny:latest  dimavz/whisper-tiny:latest   \n",
       "4             openchat:latest             openchat:latest   \n",
       "\n",
       "                           modified_at  size_bytes  \\\n",
       "0  2025-09-03T15:23:37.281393299-04:00  4920753328   \n",
       "1  2025-08-12T09:12:54.355064054-04:00  3825819449   \n",
       "2  2025-07-22T19:00:46.141884565-04:00    44039248   \n",
       "3  2025-07-22T18:50:09.444127962-04:00    44039248   \n",
       "4  2025-07-14T18:40:40.971119981-04:00  4109876386   \n",
       "\n",
       "                                              digest format   family  \\\n",
       "0  46e0c10c039e019119339687c3c1757cc81b9da49709a3...   gguf    llama   \n",
       "1  44040b9222331f7eacd27ec9254e42de585af28d2c5d12...   gguf    llama   \n",
       "2  9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81...   gguf  unknown   \n",
       "3  9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81...   gguf  unknown   \n",
       "4  537a4e03b649d93bf57381199a85f412bfc35912e46db1...   gguf    llama   \n",
       "\n",
       "    families parameter_size quantization  parameter_count  context_length  \n",
       "0    [llama]           8.0B       Q4_K_M       8030261312        131072.0  \n",
       "1       None             7B         Q4_0       6738415616          2048.0  \n",
       "2  [unknown]         37.76M      unknown         37760640             NaN  \n",
       "3  [unknown]         37.76M      unknown         37760640             NaN  \n",
       "4    [llama]             7B         Q4_0       7241748480          8192.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert and preview\n",
    "df_models = registry_to_dataframe(model_registry)\n",
    "df_models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b8119-f615-4242-a2f0-3e9a9181e5d5",
   "metadata": {},
   "source": [
    "# Expanded model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9ada74-09af-4767-81ac-5a3afd487385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama3.1:8b': {'name': 'llama3.1:8b',\n",
       "  'modified_at': '2025-09-03T15:23:37.281393299-04:00',\n",
       "  'size_bytes': 4920753328,\n",
       "  'digest': '46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '8.0B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 8030261312,\n",
       "  'context_length': 131072,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'llama2-uncensored:latest': {'name': 'llama2-uncensored:latest',\n",
       "  'modified_at': '2025-08-12T09:12:54.355064054-04:00',\n",
       "  'size_bytes': 3825819449,\n",
       "  'digest': '44040b9222331f7eacd27ec9254e42de585af28d2c5d1211cdaeb3ffa361fe3f',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': None,\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 6738415616,\n",
       "  'context_length': 2048,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'whisper:latest': {'name': 'whisper:latest',\n",
       "  'modified_at': '2025-07-22T19:00:46.141884565-04:00',\n",
       "  'size_bytes': 44039248,\n",
       "  'digest': '9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81b6dcc455efeea6b8b0',\n",
       "  'format': 'gguf',\n",
       "  'family': 'unknown',\n",
       "  'families': ['unknown'],\n",
       "  'parameter_size': '37.76M',\n",
       "  'quantization': 'unknown',\n",
       "  'parameter_count': 37760640,\n",
       "  'context_length': None,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'dimavz/whisper-tiny:latest': {'name': 'dimavz/whisper-tiny:latest',\n",
       "  'modified_at': '2025-07-22T18:50:09.444127962-04:00',\n",
       "  'size_bytes': 44039248,\n",
       "  'digest': '9aafc61ff108ca6baffe7131bbd34ad157dfaf30799f81b6dcc455efeea6b8b0',\n",
       "  'format': 'gguf',\n",
       "  'family': 'unknown',\n",
       "  'families': ['unknown'],\n",
       "  'parameter_size': '37.76M',\n",
       "  'quantization': 'unknown',\n",
       "  'parameter_count': 37760640,\n",
       "  'context_length': None,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'openchat:latest': {'name': 'openchat:latest',\n",
       "  'modified_at': '2025-07-14T18:40:40.971119981-04:00',\n",
       "  'size_bytes': 4109876386,\n",
       "  'digest': '537a4e03b649d93bf57381199a85f412bfc35912e46db197407740230968e71f',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 7241748480,\n",
       "  'context_length': 8192,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'phi3:mini': {'name': 'phi3:mini',\n",
       "  'modified_at': '2025-07-13T17:18:01.565393789-04:00',\n",
       "  'size_bytes': 2176178913,\n",
       "  'digest': '4f222292793889a9a40a020799cfd28d53f3e01af25d48e06c5e708610fc47e9',\n",
       "  'format': 'gguf',\n",
       "  'family': 'phi3',\n",
       "  'families': ['phi3'],\n",
       "  'parameter_size': '3.8B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 3821079648,\n",
       "  'context_length': 131072,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'deepseek-coder-v2:latest': {'name': 'deepseek-coder-v2:latest',\n",
       "  'modified_at': '2025-07-09T18:49:47.931980663-04:00',\n",
       "  'size_bytes': 8905126121,\n",
       "  'digest': '63fb193b3a9b4322a18e8c6b250ca2e70a5ff531e962dbf95ba089b2566f2fa5',\n",
       "  'format': 'gguf',\n",
       "  'family': 'deepseek2',\n",
       "  'families': ['deepseek2'],\n",
       "  'parameter_size': '15.7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 15706484224,\n",
       "  'context_length': 163840,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'llama2:13b': {'name': 'llama2:13b',\n",
       "  'modified_at': '2025-05-05T18:57:06.208582485-04:00',\n",
       "  'size_bytes': 7366821294,\n",
       "  'digest': 'd475bf4c50bc4d29f333023e38cd56535039eec11052204e5304c8773cc8416c',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '13B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 13015864320,\n",
       "  'context_length': 4096,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'llama2:latest': {'name': 'llama2:latest',\n",
       "  'modified_at': '2025-05-05T18:51:57.858011224-04:00',\n",
       "  'size_bytes': 3826793677,\n",
       "  'digest': '78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 6738415616,\n",
       "  'context_length': 4096,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'mistral:latest': {'name': 'mistral:latest',\n",
       "  'modified_at': '2025-04-27T14:40:11.242436536-04:00',\n",
       "  'size_bytes': 4113301824,\n",
       "  'digest': 'f974a74358d62a017b37c6f424fcdf2744ca02926c4f952513ddf474b2fa5091',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7.2B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 7248023552,\n",
       "  'context_length': 32768,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'llama3.2:latest': {'name': 'llama3.2:latest',\n",
       "  'modified_at': '2025-04-21T15:58:20.764949178-04:00',\n",
       "  'size_bytes': 2019393189,\n",
       "  'digest': 'a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '3.2B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 3212749888,\n",
       "  'context_length': 131072,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'llama3.1:latest': {'name': 'llama3.1:latest',\n",
       "  'modified_at': '2025-04-21T15:53:22.687678259-04:00',\n",
       "  'size_bytes': 4920753328,\n",
       "  'digest': '46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '8.0B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 8030261312,\n",
       "  'context_length': 131072,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'llava:latest': {'name': 'llava:latest',\n",
       "  'modified_at': '2025-04-20T15:41:19.42622029-04:00',\n",
       "  'size_bytes': 4733363377,\n",
       "  'digest': '8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama', 'clip'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 7241732096,\n",
       "  'context_length': 32768,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'mannix/llama3-uncensored:latest': {'name': 'mannix/llama3-uncensored:latest',\n",
       "  'modified_at': '2025-04-20T15:27:40.375969333-04:00',\n",
       "  'size_bytes': 4661225247,\n",
       "  'digest': '2a46886ded540156e538542414cb1abecd15d2752b30c7125993883582760600',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '8B',\n",
       "  'quantization': 'Q4_0',\n",
       "  'parameter_count': 8030261248,\n",
       "  'context_length': 8192,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'gemma3:12b': {'name': 'gemma3:12b',\n",
       "  'modified_at': '2025-04-20T15:24:14.131208347-04:00',\n",
       "  'size_bytes': 8149190253,\n",
       "  'digest': 'f4031aab637d1ffa37b42570452ae0e4fad0314754d17ded67322e4b95836f8a',\n",
       "  'format': 'gguf',\n",
       "  'family': 'gemma3',\n",
       "  'families': ['gemma3'],\n",
       "  'parameter_size': '12.2B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 12187079280,\n",
       "  'context_length': 131072,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'gemma3:1b': {'name': 'gemma3:1b',\n",
       "  'modified_at': '2025-04-20T15:19:36.679311687-04:00',\n",
       "  'size_bytes': 815319791,\n",
       "  'digest': '8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc',\n",
       "  'format': 'gguf',\n",
       "  'family': 'gemma3',\n",
       "  'families': ['gemma3'],\n",
       "  'parameter_size': '999.89M',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 999885952,\n",
       "  'context_length': 32768,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'},\n",
       " 'gemma3:latest': {'name': 'gemma3:latest',\n",
       "  'modified_at': '2025-04-20T15:19:01.77545181-04:00',\n",
       "  'size_bytes': 3338801804,\n",
       "  'digest': 'a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a',\n",
       "  'format': 'gguf',\n",
       "  'family': 'gemma3',\n",
       "  'families': ['gemma3'],\n",
       "  'parameter_size': '4.3B',\n",
       "  'quantization': 'Q4_K_M',\n",
       "  'parameter_count': 4299915632,\n",
       "  'context_length': 131072,\n",
       "  'instruction_tuned': True,\n",
       "  'task_specialty': ['general'],\n",
       "  'release_date': 'unknown',\n",
       "  'publisher': 'unknown',\n",
       "  'license': 'unknown'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enrich the registry\n",
    "model_registry = enrich_model_registry(model_registry)\n",
    "model_registry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4390d-2849-4393-887d-474b09bf1fa7",
   "metadata": {},
   "source": [
    "## Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ace45c-6499-40c6-9d95-128e8508b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models per family:\n",
      "family\n",
      "llama        10\n",
      "gemma3        3\n",
      "unknown       2\n",
      "phi3          1\n",
      "deepseek2     1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Models per task_specialty:\n",
      "general    17\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Average context length by family:\n",
      "family\n",
      "deepseek2    163840.0\n",
      "gemma3        98304.0\n",
      "llama         48537.6\n",
      "phi3         131072.0\n",
      "unknown           NaN\n",
      "Name: context_length, dtype: float64 \n",
      "\n",
      "Largest models by parameter_count:\n",
      "                       model                      name  \\\n",
      "6   deepseek-coder-v2:latest  deepseek-coder-v2:latest   \n",
      "7                 llama2:13b                llama2:13b   \n",
      "14                gemma3:12b                gemma3:12b   \n",
      "0                llama3.1:8b               llama3.1:8b   \n",
      "11           llama3.1:latest           llama3.1:latest   \n",
      "\n",
      "                            modified_at  size_bytes  \\\n",
      "6   2025-07-09T18:49:47.931980663-04:00  8905126121   \n",
      "7   2025-05-05T18:57:06.208582485-04:00  7366821294   \n",
      "14  2025-04-20T15:24:14.131208347-04:00  8149190253   \n",
      "0   2025-09-03T15:23:37.281393299-04:00  4920753328   \n",
      "11  2025-04-21T15:53:22.687678259-04:00  4920753328   \n",
      "\n",
      "                                               digest format     family  \\\n",
      "6   63fb193b3a9b4322a18e8c6b250ca2e70a5ff531e962db...   gguf  deepseek2   \n",
      "7   d475bf4c50bc4d29f333023e38cd56535039eec1105220...   gguf      llama   \n",
      "14  f4031aab637d1ffa37b42570452ae0e4fad0314754d17d...   gguf     gemma3   \n",
      "0   46e0c10c039e019119339687c3c1757cc81b9da49709a3...   gguf      llama   \n",
      "11  46e0c10c039e019119339687c3c1757cc81b9da49709a3...   gguf      llama   \n",
      "\n",
      "       families parameter_size quantization  parameter_count  context_length  \\\n",
      "6   [deepseek2]          15.7B         Q4_0      15706484224        163840.0   \n",
      "7       [llama]            13B         Q4_0      13015864320          4096.0   \n",
      "14     [gemma3]          12.2B       Q4_K_M      12187079280        131072.0   \n",
      "0       [llama]           8.0B       Q4_K_M       8030261312        131072.0   \n",
      "11      [llama]           8.0B       Q4_K_M       8030261312        131072.0   \n",
      "\n",
      "    instruction_tuned task_specialty release_date publisher  license  \n",
      "6                True      [general]      unknown   unknown  unknown  \n",
      "7                True      [general]      unknown   unknown  unknown  \n",
      "14               True      [general]      unknown   unknown  unknown  \n",
      "0                True      [general]      unknown   unknown  unknown  \n",
      "11               True      [general]      unknown   unknown  unknown  \n"
     ]
    }
   ],
   "source": [
    "# Build DataFrame\n",
    "df_models = registry_to_dataframe(model_registry)\n",
    "\n",
    "# --- Useful summaries ---\n",
    "print(\"Models per family:\")\n",
    "print(df_models[\"family\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Models per task_specialty:\")\n",
    "# task_specialty is a list, so flatten it for counts\n",
    "all_tasks = pd.Series([task for tasks in df_models[\"task_specialty\"] for task in tasks])\n",
    "print(all_tasks.value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Average context length by family:\")\n",
    "print(df_models.groupby(\"family\")[\"context_length\"].mean(), \"\\n\")\n",
    "\n",
    "print(\"Largest models by parameter_count:\")\n",
    "print(df_models.sort_values(\"parameter_count\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7813ff-bc69-410e-a7cc-9ece0cee5b8c",
   "metadata": {},
   "source": [
    "## recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6966e4bc-c591-4976-a744-5a1b0995c6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>family</th>\n",
       "      <th>parameter_count</th>\n",
       "      <th>context_length</th>\n",
       "      <th>instruction_tuned</th>\n",
       "      <th>task_specialty</th>\n",
       "      <th>quantization</th>\n",
       "      <th>score</th>\n",
       "      <th>reasons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deepseek-coder-v2:latest</td>\n",
       "      <td>deepseek2</td>\n",
       "      <td>15706484224</td>\n",
       "      <td>163840.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[code, general, long_context]</td>\n",
       "      <td>Q4_0</td>\n",
       "      <td>6.044</td>\n",
       "      <td>code-specialized; parameter_count≈15,706,484,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>llama</td>\n",
       "      <td>13015864320</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[general]</td>\n",
       "      <td>Q4_0</td>\n",
       "      <td>3.659</td>\n",
       "      <td>general model for code; parameter_count≈13,015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma3:12b</td>\n",
       "      <td>gemma3</td>\n",
       "      <td>12187079280</td>\n",
       "      <td>131072.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[general, long_context]</td>\n",
       "      <td>Q4_K_M</td>\n",
       "      <td>3.541</td>\n",
       "      <td>general model for code; parameter_count≈12,187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>llama</td>\n",
       "      <td>8030261312</td>\n",
       "      <td>131072.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[general, long_context]</td>\n",
       "      <td>Q4_K_M</td>\n",
       "      <td>2.947</td>\n",
       "      <td>general model for code; parameter_count≈8,030,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3.1:latest</td>\n",
       "      <td>llama</td>\n",
       "      <td>8030261312</td>\n",
       "      <td>131072.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[general, long_context]</td>\n",
       "      <td>Q4_K_M</td>\n",
       "      <td>2.947</td>\n",
       "      <td>general model for code; parameter_count≈8,030,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model     family  parameter_count  context_length  \\\n",
       "0  deepseek-coder-v2:latest  deepseek2      15706484224        163840.0   \n",
       "1                llama2:13b      llama      13015864320          4096.0   \n",
       "2                gemma3:12b     gemma3      12187079280        131072.0   \n",
       "3               llama3.1:8b      llama       8030261312        131072.0   \n",
       "4           llama3.1:latest      llama       8030261312        131072.0   \n",
       "\n",
       "   instruction_tuned                 task_specialty quantization  score  \\\n",
       "0               True  [code, general, long_context]         Q4_0  6.044   \n",
       "1               True                      [general]         Q4_0  3.659   \n",
       "2               True        [general, long_context]       Q4_K_M  3.541   \n",
       "3               True        [general, long_context]       Q4_K_M  2.947   \n",
       "4               True        [general, long_context]       Q4_K_M  2.947   \n",
       "\n",
       "                                             reasons  \n",
       "0  code-specialized; parameter_count≈15,706,484,2...  \n",
       "1  general model for code; parameter_count≈13,015...  \n",
       "2  general model for code; parameter_count≈12,187...  \n",
       "3  general model for code; parameter_count≈8,030,...  \n",
       "4  general model for code; parameter_count≈8,030,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimal rule-based recommender (resource-agnostic)\n",
    "\n",
    "# 1) Heuristic tagger: infer specialties/flags from model name\n",
    "def _auto_tag_model(entry):\n",
    "    \"\"\"\n",
    "    Adds/adjusts tags based on common naming patterns.\n",
    "    Safe defaults stay in place if no matches are found.\n",
    "    \"\"\"\n",
    "    name = entry.get(\"name\", \"\").lower()\n",
    "\n",
    "    # --- Defaults (explicit for visibility) ---\n",
    "    # keep any existing values from your earlier enrichment\n",
    "    entry.setdefault(\"instruction_tuned\", True)\n",
    "    entry.setdefault(\"task_specialty\", [\"general\"])\n",
    "    entry.setdefault(\"multilingual\", True)  # adjust if you prefer conservative False\n",
    "    # -----------------------------------------\n",
    "\n",
    "    tags = set(entry.get(\"task_specialty\", []))\n",
    "\n",
    "    # Code-focused models\n",
    "    if any(t in name for t in [\"codellama\", \"code\", \"qwen2.5-coder\", \"deepseek-coder\", \"starcoder\", \"phind\"]):\n",
    "        tags.add(\"code\")\n",
    "\n",
    "    # Math / reasoning hints\n",
    "    if any(t in name for t in [\"math\", \"reason\", \"deepseek-r1\", \"r1:\"]):\n",
    "        tags.add(\"reasoning\")\n",
    "\n",
    "    # Instruct/chat indicators\n",
    "    if any(t in name for t in [\"instruct\", \"chat\", \":it\", \"-it\", \"qwen2.5-instruct\", \"gemma-it\"]):\n",
    "        entry[\"instruction_tuned\"] = True\n",
    "\n",
    "    # Long-context hint (numeric check beats name)\n",
    "    if isinstance(entry.get(\"context_length\"), int) and entry[\"context_length\"] >= 64_000:\n",
    "        tags.add(\"long_context\")\n",
    "\n",
    "    # Multilingual hints (very rough)\n",
    "    if any(t in name for t in [\"qwen\", \"gemma\", \"mistral\", \"llama\", \"phi\"]):\n",
    "        entry[\"multilingual\"] = True\n",
    "\n",
    "    entry[\"task_specialty\"] = sorted(tags) if tags else [\"general\"]\n",
    "    return entry\n",
    "\n",
    "\n",
    "# Apply auto-tagging to your in-memory registry\n",
    "for _n, _e in model_registry.items():\n",
    "    model_registry[_n] = _auto_tag_model(_e)\n",
    "\n",
    "# Rebuild the DataFrame to reflect updates\n",
    "df_models = pd.DataFrame.from_dict(model_registry, orient=\"index\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 2) Recommendation function\n",
    "def recommend_model(\n",
    "    use_case: str,\n",
    "    min_context: int | None = None,\n",
    "    require_instruction_tuned: bool | None = None,\n",
    "    prefer_newer: bool = True,\n",
    "    top_k: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score and rank models for a given use_case.\n",
    "    Resource constraints are intentionally ignored.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_case : str\n",
    "        e.g., \"general_chat\", \"code_generation\", \"long_doc_summary\", \"reasoning_qa\"\n",
    "    min_context : int | None\n",
    "        Minimum context length requirement (tokens). If None, no hard floor.\n",
    "    require_instruction_tuned : bool | None\n",
    "        If True, filter to instruction-tuned; if False, prefer non-instruction; if None, no hard requirement.\n",
    "    prefer_newer : bool\n",
    "        Use 'release_date' as a tie-break preference when available.\n",
    "    top_k : int\n",
    "        Number of top candidates to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns: model, family, parameter_count, context_length, instruction_tuned,\n",
    "    task_specialty, score, reasons\n",
    "    \"\"\"\n",
    "    if df_models.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Map use_case to soft preferences\n",
    "    uc = use_case.lower()\n",
    "    wants = {\n",
    "        \"code\": uc in (\"code\", \"code_generation\", \"code_assist\", \"coding\"),\n",
    "        \"long\": uc in (\"long_context\", \"long_doc_summary\", \"summarization_long\"),\n",
    "        \"reason\": uc in (\"reasoning\", \"reasoning_qa\", \"math\", \"complex_qa\"),\n",
    "        \"chat\": uc in (\"chat\", \"assistant\", \"general_chat\", \"qa\", \"customer_support\"),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df_models.iterrows():\n",
    "        score = 0\n",
    "        reasons = []\n",
    "\n",
    "        # Basic fields\n",
    "        model = row.get(\"name\") or row.get(\"model\") or \"unknown\"\n",
    "        family = row.get(\"family\")\n",
    "        ctx = row.get(\"context_length\")\n",
    "        pc = row.get(\"parameter_count\")\n",
    "        instruct = bool(row.get(\"instruction_tuned\"))\n",
    "        specialty = set(row.get(\"task_specialty\", []) or [])\n",
    "\n",
    "        # Hard filters\n",
    "        if min_context is not None and isinstance(ctx, int) and ctx < min_context:\n",
    "            continue\n",
    "        if require_instruction_tuned is True and not instruct:\n",
    "            continue\n",
    "\n",
    "        # Soft preferences\n",
    "        # Instruction tuning for chat/assistant flows\n",
    "        if wants[\"chat\"]:\n",
    "            if instruct:\n",
    "                score += 3; reasons.append(\"instruction-tuned for chat\")\n",
    "            else:\n",
    "                score -= 1; reasons.append(\"not instruction-tuned\")\n",
    "\n",
    "        # Code specialty\n",
    "        if wants[\"code\"]:\n",
    "            if \"code\" in specialty:\n",
    "                score += 3; reasons.append(\"code-specialized\")\n",
    "            else:\n",
    "                score += 1; reasons.append(\"general model for code\")\n",
    "\n",
    "        # Reasoning/math specialty\n",
    "        if wants[\"reason\"]:\n",
    "            if \"reasoning\" in specialty:\n",
    "                score += 3; reasons.append(\"reasoning-specialized\")\n",
    "            else:\n",
    "                score += 1; reasons.append(\"general reasoning\")\n",
    "\n",
    "        # Long context preference\n",
    "        if wants[\"long\"]:\n",
    "            if isinstance(ctx, int):\n",
    "                # reward proportionally up to 128k\n",
    "                score += min(ctx, 128_000) / 32_000  # up to +4\n",
    "                reasons.append(f\"context_length={ctx}\")\n",
    "            else:\n",
    "                score -= 1; reasons.append(\"unknown context_length\")\n",
    "\n",
    "        # General capability proxy via parameter count (diminishing returns)\n",
    "        if isinstance(pc, (int, float)):\n",
    "            # scale: 1e9 -> +1, 7e9 -> +2.5, 13e9 -> +3, 70e9 -> +4 (capped)\n",
    "            cap_bonus = min(4.0, 1.0 + (pc / 7e9))  # simple, monotonic\n",
    "            score += cap_bonus\n",
    "            reasons.append(f\"parameter_count≈{int(pc):,}\")\n",
    "        else:\n",
    "            reasons.append(\"unknown parameter_count\")\n",
    "\n",
    "        # Quantization nudge (lighter quantization may reduce quality slightly)\n",
    "        q = (row.get(\"quantization\") or \"\").upper()\n",
    "        if q.startswith(\"Q2\"):\n",
    "            score -= 1; reasons.append(\"very aggressive quantization\")\n",
    "        elif q.startswith(\"Q3\"):\n",
    "            score -= 0.5; reasons.append(\"aggressive quantization\")\n",
    "        elif q.startswith(\"Q4\"):\n",
    "            score -= 0.2; reasons.append(\"moderate quantization\")\n",
    "        elif q.startswith(\"Q5\") or q.startswith(\"Q6\") or q.startswith(\"F16\") or q.startswith(\"BF16\"):\n",
    "            score += 0.2; reasons.append(\"higher quality quantization\")\n",
    "\n",
    "        # Prefer newer releases if available\n",
    "        if prefer_newer:\n",
    "            rd = row.get(\"release_date\")\n",
    "            if isinstance(rd, str) and rd not in (\"\", \"unknown\"):\n",
    "                score += 0.2; reasons.append(\"newer release\")\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"family\": family,\n",
    "            \"parameter_count\": pc,\n",
    "            \"context_length\": ctx,\n",
    "            \"instruction_tuned\": instruct,\n",
    "            \"task_specialty\": sorted(list(specialty)) if specialty else [\"general\"],\n",
    "            \"quantization\": row.get(\"quantization\"),\n",
    "            \"score\": round(float(score), 3),\n",
    "            \"reasons\": \"; \".join(reasons)\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    recs = pd.DataFrame(rows)\n",
    "    recs = recs.sort_values([\"score\", \"parameter_count\", \"context_length\"], ascending=[False, False, False])\n",
    "    return recs.head(top_k).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 3) Example calls (uncomment to try):\n",
    "# recommend_model(\"general_chat\", min_context=4096, require_instruction_tuned=True, top_k=5)\n",
    "recommend_model(\"code_generation\", min_context=8192, top_k=5)\n",
    "# recommend_model(\"long_doc_summary\", min_context=65536, top_k=5)\n",
    "# recommend_model(\"reasoning_qa\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80b76c-fbee-444e-9c08-b64d8f7803b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPU",
   "language": "python",
   "name": "gpu_enabled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
